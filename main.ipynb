{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "be4fdfb4",
      "metadata": {},
      "source": [
        "Set `COLAB` to True in the following cell if you wish to run in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d0e467",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set to True is working in Google Colab. Make sure you are running on GPU.\n",
        "COLAB = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b00dfaf",
      "metadata": {},
      "source": [
        "#### Mount Google Drive and Copy Project Directories\n",
        "This cell mounts Google Drive in Colab and copies 'toolbox' and 'data' directories from the project's Google Drive location to the Colab environment if they don't already exist."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7409563c",
      "metadata": {},
      "outputs": [],
      "source": [
        "if COLAB == True:\n",
        "    from google.colab import drive\n",
        "    import shutil\n",
        "    import os\n",
        "\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    if os.path.exists('/content/toolbox') == False:\n",
        "        shutil.copytree('/content/drive/MyDrive/CS502_project/toolbox', '/content/toolbox')\n",
        "    if os.path.exists('/content/data') == False:\n",
        "        shutil.copytree('/content/drive/MyDrive/CS502_project/data', '/content/data')\n",
        "    if os.path.exists('/content/unet++.pt') == False:\n",
        "        shutil.copy('/content/drive/MyDrive/CS502_project/unet++.pt', '/content/unet++.pt')\n",
        "    if os.path.exists('/content/resnet50.pt') == False:\n",
        "        shutil.copy('/content/drive/MyDrive/CS502_project/resnet50.pt', '/content/resnet50.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XJWiMrjqwnZo",
      "metadata": {
        "id": "XJWiMrjqwnZo"
      },
      "source": [
        "# **Fusion of U-Net++ and ResNet50 Models for Melanoma Diagnosis from Dermoscopic Images**\n",
        "\n",
        "Team 45, Kilian Zell 287515"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8PXnl_GyjUuH",
      "metadata": {
        "id": "8PXnl_GyjUuH"
      },
      "source": [
        "### PROBLEM STATEMENT\n",
        "\n",
        "Skin cancer is the most prevalent type of cancer, and while melanoma constitutes only around 1% of them, it is responsible for the majority of skin cancer-related death. Currently, melanoma diagnoses rely primarily on the visual examination of skin lesions by dermatologists. Unfortunately, this kind of diagnosis is prone to subjectivity and errors. This significantly impact patients' well-being, as an early-stage misdiagnosis can substantially diminish overall chances of survival."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sLF3UV1LkBEM",
      "metadata": {
        "id": "sLF3UV1LkBEM"
      },
      "source": [
        "### PROPOSED SOLUTION\n",
        "\n",
        "In this context, deep learning has the potential to significantly enhance the consistency and accuracy of diagnosis while remaining entirely non-invasive. In this work, a model specifically designed to classify skin lesions as 'melanoma' or 'non-melanoma' is proposed. In unsegmented dermoscopic images, the borders of a lesion and skin texture can lead to unwanted feature extraction, affecting the overall performance of classification. To overcome this challenge, the proposed model first extract the Region of Interest (ROI) form the input image using a U-Net++ inspired model so that only relevant features are considered for diagnosis. The resulting segmented region is then fed into a pre-trained ResNet50 calibrated for binary classification to obtain the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de5awOIoxPmD",
      "metadata": {
        "id": "de5awOIoxPmD"
      },
      "source": [
        "### WORKFLOW\n",
        "\n",
        "This notebook is structured into three main parts:\n",
        "\n",
        "**Part 1: Lesion Segmentation.** In this initial phase of the project, the focus is on implementing advanced image processing and machine learning techniques to achieve precise and dependable lesion segmentation from dermoscopic images. The evaluation will be based on several performance metrics, including the Dice coefficient and pixel-level accuracy.\n",
        "\n",
        "\n",
        "**Part 2: Lesion Classification.** Building upon the 'region of interest' extraction approach developed in the previous part, this part aims to predict lesion disease states, categorizing them as either 'melanoma' or 'non-melanoma'. The model performance evaluation will involve key metrics such as accuracy, specificity, sensitivity, precision, and F1-score.\n",
        "\n",
        "*---Please be aware that the following section has been implemented successfully but has not undergone thorough testing. Therefore, you may choose to disregard this section entirely.---*\n",
        "\n",
        "**Part 3: Metadata Integration.** In this section, the aim is to extend the classification efforts by incorporating additional available patient metadata (age, lesion location, and sex). The model performance evaluation will involve key metrics such as accuracy, specificity, sensitivity, precision, and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "891af926",
      "metadata": {},
      "source": [
        "The following figure summerizes the different steps of the project:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f80f7c6d",
      "metadata": {},
      "source": [
        "<img src=\"figures/fig2.png\" alt=\"Image Alt Text\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cc057eb-e59d-405d-954f-5145a2f8ca98",
      "metadata": {
        "id": "8cc057eb-e59d-405d-954f-5145a2f8ca98",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "# Environment Setup and Device Configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OKa2uT97s_ZZ",
      "metadata": {
        "id": "OKa2uT97s_ZZ"
      },
      "source": [
        "### Package Installation\n",
        "\n",
        "The provided command utilizes pip3 to install essential Python libraries for this project. These libraries include NumPy for numerical operations, Pillow for image processing, Matplotlib for data visualization, PyTorch and Torchvision for deep learning tasks, OpenCV-Python for computer vision, and additional utilities like SciPy and scikit-image. Ensure that you have the necessary Python environment set up before executing this command to ensure a seamless installation of the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to install the requirements. DO NOT un-comment if working in Colab.\n",
        "#!pip3 install numpy==1.24.4 Pillow==10.1.0 matplotlib==3.7.4 torch==2.1.1 torchvision==0.16.1 opencv-python==4.8.1.78 scipy==1.10.1 scikit-image==0.21.0 pandas==2.0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Import\n",
        "\n",
        "This code cell starts by importing essential libraries such as NumPy, operating system utilities, and PyTorch modules. Additionally, it incorporates a custom `toolbox` with modules for utility functions, dataset handling, model architectures, training procedures, and plotting tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3adf91b-05de-4cf4-a49b-96d783abbb6f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3adf91b-05de-4cf4-a49b-96d783abbb6f",
        "outputId": "1c616804-430d-4b6a-a751-520b240e5219",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "# Import PyTorch modules\n",
        "import torch\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "\n",
        "# Import custom toolbox modules\n",
        "from toolbox import utils, dataset, models, training, plots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Device Configuration and GPU Monitoring\n",
        "\n",
        "This code cell checks for the availability of a GPU and sets the device accordingly, prioritizing GPU if available; otherwise, it defaults to CPU. It also provides information on GPU memory usage.\n",
        "\n",
        "Please note that this code may need adjustments based on your computing environment. Ensure that the necessary libraries and configurations are set up, and modify the code accordingly to match your specific system.\n",
        "\n",
        "- The device variable is set to \"mps\" if Torch MPS (Multi-Process Service) is available, indicating GPU usage; otherwise, it is set to \"cpu.\"\n",
        "- An environment variable (`PYTORCH_MPS_HIGH_WATERMARK_RATIO`) is set to \"0.0\" to manage memory usage.\n",
        "- The value of the environment variable is then checked and displayed.\n",
        "- GPU memory allocation and caching information are printed to provide insights into GPU memory usage.\n",
        "\n",
        "Review and adapt this code to suit the specifications of your computing system.\n",
        "\n",
        "For Google Colab, simply de-comment the corresponding code and delete the rest of the cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device as CUDA to use GPU in Colab\n",
        "if COLAB == True:\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"| Using device                 | {device}\")\n",
        "\n",
        "else:\n",
        "    # Check and set device (GPU if available, else CPU)\n",
        "    device = \"mps\" if torch.backends.mps.is_available() else \"cpu\" \n",
        "\n",
        "    # Set the environment variable\n",
        "    os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\"\n",
        "\n",
        "    # Check the value of the environment variable\n",
        "    mps_high_watermark_ratio = os.environ.get(\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\", \"Not set\")\n",
        "\n",
        "    # Check GPU memory usage\n",
        "    gpu_memory_allocated = torch.cuda.memory_allocated() / (1024 ** 3)  # in GB\n",
        "    gpu_memory_cached = torch.cuda.memory_cached() / (1024 ** 3)  # in GB\n",
        "\n",
        "    print(\"+\" + \"-\" * 30 + \"+\")\n",
        "    print(f\"| Using device                 | {device:<20}\")\n",
        "    print(f\"| WATERMARK_RATIO              | {mps_high_watermark_ratio:<20}\")\n",
        "    print(f\"| GPU memory allocated         | {gpu_memory_allocated} GB{' ' * (20 - len(str(gpu_memory_allocated)))}\")\n",
        "    print(f\"| GPU memory cached            | {gpu_memory_cached} GB{' ' * (20 - len(str(gpu_memory_cached)))}\")\n",
        "    print(\"+\" + \"-\" * 30 + \"+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d310adba-ec50-41b3-80f4-774ca3ef9246",
      "metadata": {
        "id": "d310adba-ec50-41b3-80f4-774ca3ef9246"
      },
      "source": [
        "# PART 1: U-Net-based Skin Lesions Segmentation\n",
        "\n",
        "In this initial phase of the project, the focus is on implementing advanced image processing and machine learning techniques to achieve precise and dependable lesion segmentation from dermoscopic images. The evaluation will be based on several performance metrics, including the Dice coefficient and pixel-level accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db02335-f7ac-4599-b5b0-06cae18a2812",
      "metadata": {
        "id": "1db02335-f7ac-4599-b5b0-06cae18a2812"
      },
      "source": [
        "## 1. Dataset and Dataloader\n",
        "\n",
        "The following datasets are compatible with this part of the project:\n",
        "\n",
        "- [ISIC 2016](https://challenge.isic-archive.com/data/): Train (900 samples) and test (380 samples) datasets consisting of dermoscopic lesion images in JPEG format along with their corresponding binary mask images in PNG format.\n",
        "- [ISIC 2017](https://challenge.isic-archive.com/data/): Train (2000 samples) and test (600 samples) datasets consisting of dermoscopic lesion images in JPEG format along with their corresponding binary mask images in PNG format.\n",
        "- [ISIC 2018](https://challenge.isic-archive.com/data/): Train (2594 samples) and test (1000 samples) datasets consisting of dermoscopic lesion images in JPEG format along with their corresponding binary mask images in PNG format.\n",
        "- [HAM10000](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T):  Compiled dataset consisting of 10015 dermatoscopic images in JPEG format along with their corresponding binary mask images in PNG format.\n",
        "\n",
        "Please note that for the purpose of this work, only HAM10000 was considered for training and testing the final models."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92805e65-ed12-486e-9e0a-c2d509def944",
      "metadata": {
        "id": "92805e65-ed12-486e-9e0a-c2d509def944"
      },
      "source": [
        "### Dataset Preparation and Formatting\n",
        "\n",
        "This code cell defines the path for the dataset and specifies the dataset to be used. It also includes commands to unzip and format the dataset, with relevant comments guiding adjustments based on the user's folder structure.\n",
        "\n",
        "Please modify the `DATASET_PATH` and `DATASET_USED` variables to match your specific folder structure. Uncomment the lines related to dataset unzipping and formatting if these operations are required.\n",
        "\n",
        "- `DATASET_PATH`: Adjust this variable to the root path where your dataset is stored.\n",
        "- `DATASET_USED`: Specify the name of the dataset to be utilized.\n",
        "- `TEST_FOLDER`: Should be set to 'True' if the test dataset is provided in the data folder.\n",
        "- `SPLIT_RATIO`: If `TEST_FOLDER` is set to 'False', will split the dataset randomly in train and test dataset given the split ratio. \n",
        "- `DATA_PATH`: Complete path to the dataset is created using `os.path.join`.\n",
        "\n",
        "Ensure that the dataset is organized according to your project's specifications before running this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03a46e41-1683-4646-8815-4e6fcd69fe23",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03a46e41-1683-4646-8815-4e6fcd69fe23",
        "outputId": "cf5c5a35-0422-474c-cfeb-21ce21eca7d1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Reload the plot module using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "# Define the path for the dataset and specify the dataset to be used\n",
        "DATASET_PATH = '/content/data' # Modify to match your folder structure. For Colab: '/content/data'.\n",
        "DATASET_USED = 'HAM10000' # Change if you wish to work with a different dataset. For Colab: '/content/data'\n",
        "TEST_FOLDER = False # 'True' if the test dataset is already provided in the datafolder ('False' for HAM10000)\n",
        "SPLIT_RATIO = 0.2 # Test/Train split ratio\n",
        "\n",
        "# Create the complete path to the dataset\n",
        "DATA_PATH = os.path.join(DATASET_PATH, DATASET_USED)\n",
        "\n",
        "# Delete data foders if they already exist\n",
        "utils.delete_folder(DATA_PATH)\n",
        "\n",
        "# Uncomment the following line to unzip the dataset if needed\n",
        "utils.unzip_data(DATASET_PATH, DATASET_USED)\n",
        "\n",
        "if TEST_FOLDER == True:\n",
        "    print('The test sample is already provided and therefore the SPLIT_RATIO is not taken into account.')\n",
        "    # Format the dataset for processing\n",
        "    utils.format_dataset(DATA_PATH, task='segmentation')\n",
        "\n",
        "elif TEST_FOLDER == False:\n",
        "    # Split in train and test dataset according to the SPLIT_RATIO\n",
        "    utils.split_folders(DATA_PATH, p_test=SPLIT_RATIO) \n",
        "\n",
        "    # Format the dataset for processing\n",
        "    utils.format_dataset(DATA_PATH, task='segmentation')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b94ec22-5dd2-4f76-a05b-4e3bdae2aaf0",
      "metadata": {
        "id": "4b94ec22-5dd2-4f76-a05b-4e3bdae2aaf0"
      },
      "source": [
        "### Dataset Loading and DataLoader Creation\n",
        "\n",
        "In this code segment, the batch size for data loading is set. Subsequently, training and testing datasets are created using the `LesionDataset` class, specifying the dataset path (`DATA_PATH`), data type ('train' or 'test'), and the segmentation task.\n",
        "\n",
        "Additionally, DataLoader instances are created to facilitate efficient batch loading during both training and testing phases.\n",
        "\n",
        "- `BATCH_SIZE`: Specifies the number of samples per batch during data loading. Adjust this value based on your resource availability and model requirements.\n",
        "- `lesion_dataset_train` and `lesion_dataset_test`: Instances of the `LesionDataset` class are created for training and testing, respectively.\n",
        "- `trainloader` and `testloader`: DataLoader instances are generated for efficient batch loading during model training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3537912e-196e-47c0-95eb-12e48f4f3f30",
      "metadata": {
        "id": "3537912e-196e-47c0-95eb-12e48f4f3f30",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Reload the dataset module using importlib\n",
        "import importlib\n",
        "importlib.reload(dataset)\n",
        "\n",
        "# Set the batch size for data loading\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Create training and testing datasets using the LesionDataset class\n",
        "lesion_dataset_train = dataset.LesionDataset(DATA_PATH, TYPE='train', task='segmentation')\n",
        "lesion_dataset_test = dataset.LesionDataset(DATA_PATH, TYPE='test', task='segmentation')\n",
        "\n",
        "# Create DataLoader instances for efficient batch loading during training and testing\n",
        "trainloader = torch.utils.data.DataLoader(lesion_dataset_train, BATCH_SIZE, shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(lesion_dataset_test, BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Display the sizes of the original, training, and test datasets\n",
        "print(f'Orginal dataset:  {len(lesion_dataset_train) + len(lesion_dataset_test)}')\n",
        "print(f'Training dataset:  {len(lesion_dataset_train)}')\n",
        "print(f'Test dataset:      {len(lesion_dataset_test)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88b1873-8632-4ebb-a660-f39d5b027c5f",
      "metadata": {
        "id": "b88b1873-8632-4ebb-a660-f39d5b027c5f"
      },
      "source": [
        "### Data Transformation\n",
        "\n",
        "The following transformations are applied to the training set:\n",
        "- `TF.hflip()`: Random horizontal flipping with a probability of 50%\n",
        "- `TF.vflip()`: Random vertical flipping with a probability of 50%\n",
        "- `T.RandomRotation()`: Random rotation up to 45 degrees with a probability of 50%\n",
        "- `T.RandomAffine()`: Random zoom with a maximum of 120% with a probability of 50%\n",
        "- `add_hair()`: Add artificial hair with a probability of 75%\n",
        "- `cutmix()`: Apply CutMix data augmentation with a probability of 25%\n",
        "- `mosaic()`: Combine images in mosaic with a probability of 25%\n",
        "\n",
        "Note that proper resizing (256x256) is also performed to match the different model needs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yv3MRDHxn1ax",
      "metadata": {
        "id": "yv3MRDHxn1ax"
      },
      "source": [
        "The following code snippet randomly selects an index within the training dataset (`lesion_dataset_train`) and uses the plot_data function from the plot module to visualize the corresponding data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070522d3-1af2-4015-91df-3457d5c9381b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "070522d3-1af2-4015-91df-3457d5c9381b",
        "outputId": "54c896e0-3a41-44a5-d931-cc8e10e1bb08",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Reload the plot module using importlib\n",
        "import importlib\n",
        "importlib.reload(plots)\n",
        "\n",
        "# Generate a random index for visualization within the training dataset\n",
        "i = random.randint(0, len(lesion_dataset_train) - 1)\n",
        "\n",
        "# Plot the data using the plot_data function from the plot module\n",
        "plots.plot_data_PART1(lesion_dataset_train, i)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gqs0HreU2fy9",
      "metadata": {
        "id": "gqs0HreU2fy9"
      },
      "source": [
        "## 2. Segmentation Model Configuration and Initialization\n",
        "\n",
        "This code segment defines the chosen model (`MODEL`) and sets the succession of filters (`FILTER`) based on the selected architecture.\n",
        "\n",
        "The script then chooses and initializes the specified model according to the selected architecture:\n",
        "\n",
        "- `MODEL`: Represents the chosen model architecture. Modify this variable to select the desired model (e.g., 'random', 'constant', 'simple_cnn', 'unet', or 'unet++').\n",
        "- `FILTER`: Specifies the succession of filters, determining the model's complexity. Adjust these values based on your specific model requirements (you may also neeed to resize your input images accordingly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "v5XZDKJr2gBn",
      "metadata": {
        "id": "v5XZDKJr2gBn"
      },
      "outputs": [],
      "source": [
        "# Reload the model module using importlib\n",
        "import importlib\n",
        "importlib.reload(models)\n",
        "\n",
        "# Define the chosen model and set the filter numbers\n",
        "MODEL = 'unet++' # Change according to the model you want to train\n",
        "FILTER = [8, 16, 32, 64, 128] #Set the succession of filters\n",
        "\n",
        "# Choose and initialize the specified model based on the selected architecture\n",
        "if MODEL == 'random':\n",
        "    model = models.RandomGuessModel().to(device)\n",
        "elif MODEL == 'constant':\n",
        "    model = models.ConstantModel().to(device)\n",
        "elif MODEL == 'simple_cnn':\n",
        "    model = models.SimpleCNN().to(device)\n",
        "elif MODEL == 'unet':\n",
        "    model = models.UNet(filter_num=FILTER).to(device)\n",
        "elif MODEL == 'unet++':\n",
        "    model = models.UNetPlusPlus(filter_num=FILTER).to(device)\n",
        "else:\n",
        "    raise ValueError('Please choose one of the following models: random, constant, simple_cnn, unet, or unet++')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Is9rXFzk2ueX",
      "metadata": {
        "id": "Is9rXFzk2ueX"
      },
      "source": [
        "## 3. Model Training\n",
        "\n",
        "In this script users can specify whether to load a pre-trained model (`LOAD`) and set the model name accordingly.\n",
        "\n",
        "The Trainer is then initialized with the specified model and device, and based on the `LOAD` condition, either a new model is trained or a pre-trained model is loaded.\n",
        "\n",
        "- If `LOAD` is set to `False`, the script defines parameters for training, such as the number of epochs (`EPOCHS`), learning rate (`LEARNING_RATE`), weight decay (`WEIGHT_DECAY`), and gradient clipping (`CLIP_GRADIENT`). The model is then trained for the specified number of epochs unless it's the 'random' or 'constant' model. Training and testing losses are recorded.\n",
        "\n",
        "- If `LOAD` is set to `True`, a pre-trained model with the specified name (`MODEL_NAME`) is loaded using the `trainer.load_model` function.\n",
        "\n",
        "In either case, informative messages are printed, indicating the completion of the training process or the successful loading of a pre-trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fnkZb7qK2xf_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        },
        "id": "fnkZb7qK2xf_",
        "outputId": "68c7bd44-db5e-4d32-c441-1e3d076d0320"
      },
      "outputs": [],
      "source": [
        "# Reload the training module using importlib\n",
        "import importlib\n",
        "importlib.reload(training)\n",
        "\n",
        "# Specify whether or not to load the pre-trained model\n",
        "LOAD = True\n",
        "\n",
        "# Set the model name\n",
        "MODEL_NAME = f'{MODEL}.pt'\n",
        "\n",
        "# Initialize the Trainer with the specified model and device\n",
        "trainer = training.Trainer(model, device, task='segmentation')\n",
        "\n",
        "if LOAD == False:\n",
        "    # Set the number of training epochs\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 1e-4\n",
        "    WEIGHT_DECAY = 0.\n",
        "    CLIP_GRADIENT = False\n",
        "\n",
        "    # Train the model if it's not the random model\n",
        "    if MODEL != 'random' and MODEL != 'constant':\n",
        "      # Train the model for the specified number of epochs\n",
        "      losses_train, losses_test = trainer.train(EPOCHS, trainloader, testloader, model_name=MODEL_NAME, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY, clip_gradient=CLIP_GRADIENT)\n",
        "\n",
        "    # Display a message indicating the completion of the training process\n",
        "    print(f'Training finished!')\n",
        "\n",
        "else:\n",
        "    # Load a pre-trained model\n",
        "    trainer.load_model(MODEL_NAME)\n",
        "    print(f'Pre-trained model {MODEL_NAME} loaded successfuly.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CDIyj7MbpeY3",
      "metadata": {
        "id": "CDIyj7MbpeY3"
      },
      "source": [
        "### Loss Visualization\n",
        "\n",
        "This section checks if the selected model is not a benchmark model or a pre-trined model. \n",
        "\n",
        "- If the model is not a benchmark or pre-trained model (`LOAD == False`), the script plots the training and testing loss curves using the `plot_loss` function from the `plots` module.\n",
        "\n",
        "- For benchmark models ('random' or 'constant') or pre-trained models, a message is printed indicating that loss plots are not available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "npxZQE8nOd54",
      "metadata": {
        "id": "npxZQE8nOd54"
      },
      "outputs": [],
      "source": [
        "# Reload the plot module using importlib\n",
        "import importlib\n",
        "importlib.reload(plots)\n",
        "\n",
        "# Check if the model is not a benchmark model (neither random nor constant)\n",
        "if MODEL != 'random' and MODEL != 'constant':\n",
        "  if LOAD is False:\n",
        "    # Plot the training and testing loss curves using the plot_loss_unet function\n",
        "    plots.plot_loss(losses_train, losses_test, save_path=None)\n",
        "  else:\n",
        "    # Print a message indicating that loss plots are not available for pre-loaded models\n",
        "    print('Loss plot not available for pre-loaded models (please see the associated report).')\n",
        "else:\n",
        "  # Print a message indicating that loss plots are not available for benchmark models\n",
        "  print('Loss plot not available for benchmark models.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "l9ZlSwEb2x3_",
      "metadata": {
        "id": "l9ZlSwEb2x3_"
      },
      "source": [
        "## 4. Model Evaluation and Performance Scores\n",
        "\n",
        "This script checks whether the selected model is not a benchmark model (neither 'random' nor 'constant').\n",
        "\n",
        "- If the model is not a benchmark, the script proceeds to load the state dictionary from the saved model. Depending on the availability of a GPU, the model is loaded directly if a GPU is present; otherwise, it is loaded onto the CPU using `map_location`.\n",
        "\n",
        "- Subsequently, the model's performance is evaluated, and scores are obtained using the `get_scores_CLASS` function from the `utils` module. The evaluation involves comparing model predictions on the train and test dataset (`lesion_dataset_train` and `lesion_dataset_test`) against the ground truth labels. The specified threshold (`THRESHOLD`) is used to classify predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D5YrMkRE1OZ9",
      "metadata": {
        "id": "D5YrMkRE1OZ9"
      },
      "outputs": [],
      "source": [
        "# Reload the utils module using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# Check if the model is not a benchmark model (neither random nor constant)\n",
        "if MODEL != 'random' and MODEL != 'constant':\n",
        "  # Load the state dictionary from the saved model, use map_location to load on the CPU\n",
        "  model = torch.load(MODEL_NAME, map_location=torch.device('cpu'))\n",
        "\n",
        "# Evaluate the model's performance and obtain scores using the get_scores function\n",
        "#utils.get_scores_SEG(trainer, lesion_dataset_test, dataset_train=lesion_dataset_train, threshold=THRESHOLD)\n",
        "utils.get_scores_SEG(trainer, lesion_dataset_test, dataset_train=None, threshold=THRESHOLD) # Set 'dataset_train = lesion_dataset_train' to get the training metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V1FmmK_z1WS_",
      "metadata": {
        "id": "V1FmmK_z1WS_"
      },
      "source": [
        "### Model Prediction Visualization\n",
        "\n",
        "This code cell generates a random index for visualization within the testing dataset, retrieves a sample from the dataset and makes a prediction.\n",
        "\n",
        "- `i`: A random index is generated to select a sample from the testing dataset.\n",
        "- `sample`: Retrieves the corresponding sample from the testing dataset.\n",
        "- `image`, `mask`, and `output`: Prediction is made using the trained model with a specified probability threshold.\n",
        "- `title`: A title is set for visualization.\n",
        "- The `show_prediction` function from the `plots` module is then used to display the input image, ground truth mask, and model output.\n",
        "\n",
        "Adjust the index (`i`) and probability threshold as needed, and ensure that the `plots` module is correctly defined before running this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NlIIdowp2z98",
      "metadata": {
        "id": "NlIIdowp2z98"
      },
      "outputs": [],
      "source": [
        "# Reload the utils and training modules using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "importlib.reload(training)\n",
        "\n",
        "# Generate a random index for visualization within the testing dataset\n",
        "i = random.randint(0, len(lesion_dataset_test) - 1)\n",
        "\n",
        "# Retrieve a sample from the testing dataset\n",
        "sample = lesion_dataset_test[i]\n",
        "\n",
        "# Make a prediction using the trained model\n",
        "image, mask, output = trainer.predict(sample, threshold=THRESHOLD)\n",
        "\n",
        "# Set the title for visualization\n",
        "title = f'Name: {i}.png'\n",
        "\n",
        "# Display the input image, ground truth mask, and model output using the show_prediction function\n",
        "plots.show_prediction(image, mask, output[0], title, save_path=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zKwaEcnsfrqe",
      "metadata": {
        "id": "zKwaEcnsfrqe"
      },
      "source": [
        "### Output Post-Processing\n",
        "\n",
        "This code cell applies the `keep_main_region` function. The function is used to remove any isolated positive detection from the final prediction and to remove negative instances in the main positive area.\n",
        "\n",
        "The `keep_main_region` function is defined in the `dataset` module and performs the following steps:\n",
        "  - Converts the input PyTorch tensor to a NumPy array.\n",
        "  - Converts the output to uint8 and a single-channel image.\n",
        "  - Finds contours in the output and identifies the contour with the largest area.\n",
        "  - Sets to '1' anything that is inside the largest area.\n",
        "  - Sets to '0' anything that is outside the largest area.\n",
        "\n",
        "The resulting `new_output` is then visualized alongside the original mask and initial model output using the `show_post_processing` function from the `plots` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qX9zzX_fciK5",
      "metadata": {
        "id": "qX9zzX_fciK5"
      },
      "outputs": [],
      "source": [
        "# Reload the utils module using importlib\n",
        "import importlib\n",
        "importlib.reload(dataset)\n",
        "\n",
        "# Apply the keep_main_region function from the reloaded utils module\n",
        "new_output = dataset.keep_main_region(output[0])\n",
        "\n",
        "# Visualize the original mask, the initial output, and the new_output\n",
        "plots.show_post_processing(mask, output[0], new_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DQRzY-1qfslV",
      "metadata": {
        "id": "DQRzY-1qfslV"
      },
      "source": [
        "### Final Model Evaluation and Performance Scores\n",
        "\n",
        "This code snippet checks whether the selected model is not a benchmark model (neither 'random' nor 'constant').\n",
        "\n",
        "- If the model is not a benchmark, it proceeds to load the trained model from the saved file (`MODEL_NAME`).\n",
        "- Subsequently, the model's performance is evaluated, and scores are obtained using the `get_scores_SEG` function from the `utils` module.\n",
        "- The evaluation involves comparing model predictions on the training and testing dataset (`lesion_dataset_train` and `lesion_dataset_test`) against the ground truth masks. The specified threshold (`THRESHOLD`) is used to classify predictions.\n",
        "- The `processing` parameter is set to `True` to enable the additional post-processing step described above to be applied to the final prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E-WjYun1dRJg",
      "metadata": {
        "id": "E-WjYun1dRJg"
      },
      "outputs": [],
      "source": [
        "# Reload the utils module using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "# Check if the model is not a benchmark model (neither random nor constant)\n",
        "if MODEL != 'random' and MODEL != 'constant':\n",
        "  # If no GPU is available, use map_location to load on the CPU\n",
        "  model = torch.load(MODEL_NAME, map_location=torch.device('cpu'))\n",
        "\n",
        "# Evaluate the model's performance and obtain scores using the get_scores function\n",
        "utils.get_scores_SEG(trainer, lesion_dataset_test, dataset_train=None, threshold=THRESHOLD, processing=True) # Set 'dataset_train = lesion_dataset_train' to get the training metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "el-p5itWwZCN",
      "metadata": {
        "id": "el-p5itWwZCN"
      },
      "source": [
        "# PART 2: Lesion Binary Classification\n",
        "Building upon the 'region of interest' detection approach developed in the previous part, this part aims to predict lesion disease states, categorizing them as either 'melanoma' or 'non-melanoma'. The model performance evaluation involves key metrics such as accuracy, specificity, sensitivity, precision, and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37m-begkxKhv",
      "metadata": {
        "id": "37m-begkxKhv"
      },
      "source": [
        "## 1. Dataset and Dataloader\n",
        "\n",
        "The following datasets are compatible with this part of the project:\n",
        "\n",
        "- [HAM10000](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T):  Compiled dataset consisting of 10'015 dermatoscopic images in JPEG format along with their corresponding binary mask images in PNG format and entries indicating gold standard malignant status (skin lesion classification in 7 standard categories).\n",
        "- [ISIC2019](https://challenge.isic-archive.com/data/#2019):  Compiled dataset consisting of 25'331 dermatoscopic images in JPEG format along with entries indicating gold standard malignant status (skin leasion classification in 9 standard categories).\n",
        "\n",
        "Please note that for the purpose of this work, only HAM10000 was considered for training and testing the final models. Furthermore, since this work focuses on binary classification for melanoma diagnosis, only the melanoma label was considered."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preparation and Formatting\n",
        "\n",
        "This code cell defines the path for the dataset and specifies the dataset to be used. It also includes commands to unzip and format the dataset, with relevant comments guiding adjustments based on the user's folder structure.\n",
        "\n",
        "Please modify the `DATASET_PATH` and `DATASET_USED` variables to match your specific folder structure. Uncomment the lines related to dataset unzipping and formatting if these operations are required.\n",
        "\n",
        "- `DATASET_PATH`: Adjust this variable to the root path where your dataset is stored.\n",
        "- `DATASET_USED`: Specify the name of the dataset to be utilized.\n",
        "- `BENCHMARK_SEG`: This can be set to 'True' while working with `HAM10000` to use the groundtruth mask instead of segmentation predictions.\n",
        "- `DATA_PATH`: Complete path to the dataset is created using `os.path.join`.\n",
        "\n",
        "Ensure that the dataset is organized according to your project's specifications before running this code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Dug4_ESxxhRi",
      "metadata": {
        "id": "Dug4_ESxxhRi"
      },
      "outputs": [],
      "source": [
        "# Reload the model module using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "# Define the path for the dataset and specify the dataset to be used\n",
        "DATASET_PATH = '/content/data' # Change if you wish to work with a different dataset. For Colab: '/content/data'.\n",
        "DATASET_USED = 'HAM10000' # Change i you wish to work with a diffent dataset\n",
        "BENCHMARK_SEG = False # Set to 'True' while working with `HAM10000` to use the groundtruth mask for segmentation\n",
        "SPLIT_RATIO = 0.2 # Test/Train split ratio\n",
        "\n",
        "# Create the complete path to the dataset\n",
        "DATA_PATH = os.path.join(DATASET_PATH, DATASET_USED)\n",
        "\n",
        "# Delete data foders if they already exist\n",
        "utils.delete_folder(DATA_PATH)\n",
        "\n",
        "# Uncomment the following line to unzip the dataset if needed \n",
        "utils.unzip_data(DATASET_PATH, DATASET_USED)\n",
        "\n",
        "if BENCHMARK_SEG == False:\n",
        "    # Split in train and test dataset according to the SPLIT_RATIO\n",
        "    utils.split_folders(DATA_PATH, p_test=SPLIT_RATIO, type='classification', seed=11) \n",
        "elif BENCHMARK_SEG == True:\n",
        "    # Split in train and test dataset according to the SPLIT_RATIO\n",
        "    utils.split_folders(DATA_PATH, p_test=SPLIT_RATIO, type='classification_benchmark', seed=11) \n",
        "\n",
        "# Format the dataset for processing\n",
        "utils.format_dataset(DATA_PATH, task='classification', test_folder=True, benchmark=BENCHMARK_SEG)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0vdNCzxFzvHo",
      "metadata": {
        "id": "0vdNCzxFzvHo"
      },
      "source": [
        "### Defining the Segmentation Model\n",
        "\n",
        "This following code snippet, defines a pre-trained segmentation model, and initializes a trainer for that model. It allows you to choose between different segmentation models (simple_cnn, unet, or unet++) and set specific filter compositions. The code then instantiates the chosen model, and loads a pre-trained segmentation model for further use (prost-proceesing is automatically applied to segmentation predictions). Adjust the segmentation model choice and parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qYki691UzNVR",
      "metadata": {
        "id": "qYki691UzNVR"
      },
      "outputs": [],
      "source": [
        "# Reload the model module using importlib\n",
        "import importlib\n",
        "importlib.reload(models)\n",
        "importlib.reload(training)\n",
        "\n",
        "# Define the chosen model and set the filter numbers\n",
        "SEG_MODEL = 'unet++' # Change according to the model you want to use as segmentor\n",
        "SEG_MODEL_NAME = f'{SEG_MODEL}.pt'\n",
        "filter_num = [8, 16, 32, 64, 128] # Set filter composition\n",
        "\n",
        "# Choose and initialize the specified model based on the selected architecture\n",
        "if SEG_MODEL == 'simple_cnn':\n",
        "    model_seg = models.SimpleCNN().to(device)\n",
        "elif SEG_MODEL == 'unet':\n",
        "    model_seg = models.UNet(filter_num).to(device)\n",
        "elif SEG_MODEL == 'unet++':\n",
        "    model_seg = models.UNetPlusPlus(filter_num).to(device)\n",
        "else:\n",
        "    raise ValueError('Please choose one of the following models: simple_cnn, unet, or unet++')\n",
        "\n",
        "# Initialize the segmentation trainner\n",
        "trainer_seg = training.Trainer(model_seg, device)\n",
        "\n",
        "# Load the pre-trained segmentation model\n",
        "trainer_seg.load_model(SEG_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hizJUMZbx6qT",
      "metadata": {
        "id": "hizJUMZbx6qT"
      },
      "source": [
        "### Data Splitting\n",
        "\n",
        "In this code snippet, the batch size for data loading is set, and a `LesionDataset` is created for a classification test task. The dataset is then randomly split into training and testing sets with an given ratio. Finally, the sizes of the original, training, and test datasets are displayed for quick reference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V_lk05BXyBI8",
      "metadata": {
        "id": "V_lk05BXyBI8"
      },
      "outputs": [],
      "source": [
        "# Reload the dataset module using importlib\n",
        "import importlib\n",
        "importlib.reload(dataset)\n",
        "\n",
        "if BENCHMARK_SEG == True:\n",
        "    # Create training and testing datasets using the LesionDataset class\n",
        "    lesion_dataset_train = dataset.LesionDataset(DATA_PATH, TYPE='train', task='classification_benchmark', trainer_seg=trainer_seg)\n",
        "    lesion_dataset_test = dataset.LesionDataset(DATA_PATH, TYPE='test', task='classification_benchmark', trainer_seg=trainer_seg)\n",
        "elif BENCHMARK_SEG == False:\n",
        "    # Create training and testing datasets using the LesionDataset class\n",
        "    lesion_dataset_train = dataset.LesionDataset(DATA_PATH, TYPE='train', task='classification', trainer_seg=trainer_seg)\n",
        "    lesion_dataset_test = dataset.LesionDataset(DATA_PATH, TYPE='test', task='classification', trainer_seg=trainer_seg)\n",
        "\n",
        "# Display the sizes of the original, training, and test datasets\n",
        "print(\"+\" + \"-\" * 30 + \"+\")\n",
        "print(f\"| Original dataset            | {len(lesion_dataset_train) + len(lesion_dataset_test)}{' ' * (20 - len(str(len(lesion_dataset_train) + len(lesion_dataset_test))))}\")\n",
        "print(f\"| Training dataset            | {len(lesion_dataset_train)}{' ' * (20 - len(str(len(lesion_dataset_train))))}\")\n",
        "print(f\"| Test dataset                | {len(lesion_dataset_test)}{' ' * (20 - len(str(len(lesion_dataset_test))))}\")\n",
        "print(\"+\" + \"-\" * 30 + \"+\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resampling Strategy and DataLoader Configuration\n",
        "\n",
        "This code segment defines and implements a resampling strategy for dataset balancing. Depending on the specified resampling strategy, the code creates DataLoaders either with or without the use of WeightedRandomSampler.\n",
        "\n",
        "- When `RESAMPLING` is set to `None`, DataLoaders are created without resampling, utilizing a regular shuffle strategy.\n",
        "- In the case of `RESAMPLING` being set to 'upsampling', a WeightedRandomSampler is employed to balance the dataset. The weights for each class are calculated, and a WeightedRandomSampler is created using these weights for the training set. A regular DataLoader is used for the test set.\n",
        "- `RESAMPLING_RATIO` can be used to manipulate the upsampling ratio (`RESAMPLING_RATIO = 1` corresponds to a 1:1 sampling).\n",
        "- The code concludes by displaying the label ratio in the training set and printing the label distribution in the first and last 5 batches of the training set.\n",
        "\n",
        "When selected, the upsampling strategy is exclusively applied to the training set to avoid any potential bias introduced during testing. The test set remains unchanged, ensuring the evaluation reflects the original distribution of classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "# Define the resampling strategy ('upsampling' or None) and the resampling ratio\n",
        "RESAMPLING = 'upsampling'\n",
        "RESAMPLING_RATIO = 1\n",
        "BATCH_SIZE = 16 # Set the batch size for data loading\n",
        "\n",
        "if RESAMPLING is None:\n",
        "    # No resampling - create DataLoader without WeightedRandomSampler\n",
        "    trainloader = torch.utils.data.DataLoader(lesion_dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(lesion_dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "elif RESAMPLING == 'upsampling':\n",
        "    # Upsampling strategy to balance the dataset using WeightedRandomSampler\n",
        "    # Get labels from the training dataset\n",
        "    labels = utils.get_labels(lesion_dataset_train)\n",
        "\n",
        "    # Count the number of samples in each class\n",
        "    class_sample_count = np.array([len(np.where(labels == t)[0]) for t in np.unique(labels)])\n",
        "\n",
        "    # Calculate weights for each class to balance the dataset\n",
        "    weight = 1. / class_sample_count\n",
        "    weight[-1] = weight[-1] * RESAMPLING_RATIO\n",
        "\n",
        "    # Compute sample weights for each instance in the training set\n",
        "    samples_weight = np.array([weight[t] for t in labels])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "\n",
        "    # Create a WeightedRandomSampler using the calculated weights\n",
        "    sampler = WeightedRandomSampler(samples_weight, int(len(samples_weight)), replacement=True)\n",
        "\n",
        "    # Create DataLoader with WeightedRandomSampler for training set and regular DataLoader for test set\n",
        "    trainloader = torch.utils.data.DataLoader(lesion_dataset_train, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "    testloader = torch.utils.data.DataLoader(lesion_dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Display the ratio of labels in the training set\n",
        "total_labels = len(labels)\n",
        "ratio_1 = len(np.where(labels == 1)[0])\n",
        "ratio_0 = len(np.where(labels == 0)[0])\n",
        "\n",
        "print(f\"Training label ratio '1'/'0': {ratio_1}/{ratio_0}\")\n",
        "print(f'Total number of batch: {len(trainloader)}')\n",
        "\n",
        "# Print the label distribution in the first 4 batches of the training set\n",
        "print(\"Batch Index   | Ratio '1'   | Ratio '0'\")\n",
        "print(\"---------------------------------------\")\n",
        "for i, data in enumerate(itertools.islice(trainloader, 5)):\n",
        "        gt = data['label'].to(device).float()\n",
        "        ratio_1_batch = (gt == 1).sum().item()\n",
        "        ratio_0_batch = (gt == 0).sum().item()\n",
        "        print(f\"Batch {i+1:<11} | {ratio_1_batch:<11} | {ratio_0_batch:<11}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39KC54ZXyUhR",
      "metadata": {
        "id": "39KC54ZXyUhR"
      },
      "source": [
        "### Data Transformation\n",
        "\n",
        "The following transformations are applied to the training set:\n",
        "- `TF.hflip()`: Random horizontal flipping with a probability of 50%\n",
        "- `TF.vflip()`: Random vertical flipping with a probability of 50%\n",
        "- `T.RandomRotation()`: Random rotation up to 45 degrees with a probability of 50%\n",
        "- `T.RandomAffine()`: Random zoom with a maximum of 120% with a probability of 50%\n",
        "- `roi(seg_model)`: That extracts the region of interest from the original image using the segmentation model (or the groundtruth if `BENCHMARK_SEG == True`)\n",
        "- `add_hair()`: Add artificial hair with a probability of 50%\n",
        "- `transforms.ColorJitter()`: Adds controlled variability to the images by randomly adjusting their brightness, contrast, saturation, and hue with a probability of 50%\n",
        "- `T.RandomErasing()`: Applies random erasing to input images with a probability of 25%\n",
        "\n",
        "Note that proper resizing (224x224) is also performed to match the different model needs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hZH3WLVeyVLd",
      "metadata": {
        "id": "hZH3WLVeyVLd"
      },
      "source": [
        "The following code snippet generates random indices for both the training and test datasets. Subsequently, it employs the `plot_data_PART2` function from the `plots` module to visualize data corresponding to the randomly selected indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kI2rCtZuyv8n",
      "metadata": {
        "id": "kI2rCtZuyv8n"
      },
      "outputs": [],
      "source": [
        "# Reload the dataset module using importlib\n",
        "import importlib\n",
        "importlib.reload(plots)\n",
        "\n",
        "index_train = random.randint(0, len(lesion_dataset_train) - 1)\n",
        "index_test = random.randint(0, len(lesion_dataset_test) - 1)\n",
        "\n",
        "# Plot the data using the plot_data function from the plot module\n",
        "plots.plot_data_PART2(lesion_dataset_train, lesion_dataset_test, index_train, index_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iQ-XjiNx00z9",
      "metadata": {
        "id": "iQ-XjiNx00z9"
      },
      "source": [
        "## 2. Classifier Model Configuration and Initialization\n",
        "\n",
        "In this section, the chosen model architecture with the option to customize the dropout rate.\n",
        "\n",
        "The script proceeds to initialize the specified model based on the chosen architecture (`MODEL`). Depending on the selected model, including options like 'random,' 'constant', or various ResNet configurations, the corresponding model instance is created and moved to the specified computing device.\n",
        "\n",
        "`DROPOUT` can be used to introduce dropout using `nn.Dropout(p=DROPOUT)` at the model level."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VyE7IXMy1dy9",
      "metadata": {
        "id": "VyE7IXMy1dy9"
      },
      "outputs": [],
      "source": [
        "# Reload the model module using importlib\n",
        "import importlib\n",
        "importlib.reload(models)\n",
        "\n",
        "# Define the chosen model and set the filter numbers\n",
        "MODEL = 'resnet50' # Change according to the model you want to train\n",
        "DROPOUT = 0.\n",
        "\n",
        "# Choose and initialize the specified model based on the selected architecture\n",
        "if MODEL == 'random':\n",
        "    model = models.RandomGuessModel(task='classification').to(device)\n",
        "elif MODEL == 'constant':\n",
        "    model = models.ConstantModel(task='classification').to(device)\n",
        "elif 'resnet18' in MODEL:\n",
        "    model = models.ResNet18(p_dropout=DROPOUT).to(device)\n",
        "elif 'resnet34' in MODEL:\n",
        "    model = models.ResNet34(p_dropout=DROPOUT).to(device)\n",
        "elif 'resnet50' in MODEL:\n",
        "    model = models.ResNet50(p_dropout=DROPOUT).to(device)\n",
        "elif 'resnet101' in MODEL:\n",
        "    model = models.ResNet101(p_dropout=DROPOUT).to(device)\n",
        "else:\n",
        "    raise ValueError('Please choose one of the following models: random, constant, resnet18, resnet34, resnet50, resnet101 or resnet152.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HC25zkiK14Im",
      "metadata": {
        "id": "HC25zkiK14Im"
      },
      "source": [
        "## 3. Model Training\n",
        "\n",
        "In this script users can specify whether to load a pre-trained model (`LOAD`) and set the model name accordingly.\n",
        "\n",
        "The Trainer is then initialized with the specified model and device, and based on the `LOAD` condition, either a new model is trained or a pre-trained model is loaded.\n",
        "\n",
        "- If `LOAD` is set to `False`, the script defines parameters for training, such as the number of epochs (`EPOCHS`), learning rate (`LEARNING_RATE`), weight decay (`WEIGHT_DECAY`), and gradient clipping (`CLIP_GRADIENT`). The model is then trained for the specified number of epochs unless it's the 'random' or 'constant' model. Training and testing losses are recorded.\n",
        "\n",
        "- If `LOAD` is set to `True`, a pre-trained model with the specified name (`MODEL_NAME`) is loaded using the `trainer.load_model` function.\n",
        "\n",
        "In either case, informative messages are printed, indicating the completion of the training process or the successful loading of a pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m7w523Zk2HwW",
      "metadata": {
        "id": "m7w523Zk2HwW"
      },
      "outputs": [],
      "source": [
        "# Reload the training module using importlib\n",
        "import importlib\n",
        "importlib.reload(training)\n",
        "\n",
        "# Specify whether or not to load the pre-trained model\n",
        "LOAD = True\n",
        "\n",
        "# Set the model name\n",
        "MODEL_NAME = f'{MODEL}.pt'\n",
        "\n",
        "# Initialize the Trainer with the specified model and device\n",
        "trainer = training.Trainer(model, device, task='classification')\n",
        "\n",
        "if LOAD == False:\n",
        "    # Set the number of training epochs\n",
        "    EPOCHS = 20\n",
        "    LEARNING_RATE = 5e-5\n",
        "    WEIGHT_DECAY = 0.\n",
        "    CLIP_GRADIENT = True\n",
        "\n",
        "    # Train the model if it's not the random model\n",
        "    if MODEL != 'random' and MODEL != 'constant':\n",
        "      # Train the model for the specified number of epochs\n",
        "      losses_train, losses_test = trainer.train(EPOCHS, trainloader, testloader=testloader, model_name=MODEL_NAME, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY, clip_gradient=CLIP_GRADIENT)\n",
        "\n",
        "    # Display a message indicating the completion of the training process\n",
        "    print(f'Training finished!')\n",
        "\n",
        "else:\n",
        "    # Load a pre-trained model\n",
        "    trainer.load_model(MODEL_NAME)\n",
        "    print(f'Pre-trained model {MODEL_NAME} loaded successfuly.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qay2hhUc2aqt",
      "metadata": {
        "id": "qay2hhUc2aqt"
      },
      "source": [
        "### Loss Visualization\n",
        "\n",
        "This section checks if the selected model is not a benchmark model (neither 'random' nor 'constant'). \n",
        "\n",
        "- If the model is not a benchmark and training is not based on a pre-trained model (`LOAD == False`), the script plots the training and testing loss curves using the `plot_loss` function from the `plots` module.\n",
        "\n",
        "- For benchmark models ('random' or 'constant') or pre-trained models, a message is printed indicating that loss plots are not available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eXcakAgZ2buA",
      "metadata": {
        "id": "eXcakAgZ2buA"
      },
      "outputs": [],
      "source": [
        "# Reload the plot module using importlib\n",
        "import importlib\n",
        "importlib.reload(plots)\n",
        "\n",
        "# Check if the model is not a benchmark model (neither random nor constant)\n",
        "if MODEL != 'random' and MODEL != 'constant':\n",
        "  if LOAD == False:\n",
        "    # Plot the training and testing loss curves using the plot_loss_unet function\n",
        "    plots.plot_loss(losses_train, losses_test, save_path=None)\n",
        "  else:\n",
        "    # Print a message indicating that loss plots are not available for pre-loaded models\n",
        "    print('Loss plot not available for pre-loaded models (please see the associated report).')\n",
        "else:\n",
        "  # Print a message indicating that loss plots are not available for benchmark models\n",
        "  print('Loss plot not available for benchmark models.')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pbRe4ssn2uCN",
      "metadata": {
        "id": "pbRe4ssn2uCN"
      },
      "source": [
        "## 4. Model Evaluation and Performance Scores\n",
        "\n",
        "This script checks whether the selected model is not a benchmark model (neither 'random' nor 'constant').\n",
        "\n",
        "- If the model is not a benchmark, the script proceeds to load the state dictionary from the saved model. Depending on the availability of a GPU, the model is loaded directly if a GPU is present; otherwise, it is loaded onto the CPU using `map_location`.\n",
        "\n",
        "- Subsequently, the model's performance is evaluated, and scores are obtained using the `get_scores_CLASS` function from the `utils` module. The evaluation involves comparing model predictions on the train and test dataset (`lesion_dataset_train` and `lesion_dataset_test`) against the ground truth labels. The specified threshold (`THRESHOLD`) is used to classify predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bi-ETFZg21fs",
      "metadata": {
        "id": "bi-ETFZg21fs"
      },
      "outputs": [],
      "source": [
        "# Reload the utils module using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# Check if the model is not a benchmark model (neither random nor constant)\n",
        "if MODEL != 'random' and MODEL != 'constant':\n",
        "  # Load the state dictionary from the saved model, use map_location to load on the CPU\n",
        "  model = torch.load(MODEL_NAME, map_location=torch.device('cpu'))\n",
        "\n",
        "# Evaluate the model's performance and obtain scores using the get_scores function\n",
        "utils.get_scores_CLASS(trainer, lesion_dataset_test, dataset_train=None, threshold=THRESHOLD) # Set 'dataset_train = lesion_dataset_train' to get the training metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*---Please be aware that the following section has been implemented successfully but has not undergone thorough testing. Therefore, you may choose to disregard this section entirely.---*\n",
        "\n",
        "# PART 3: Metadata Integration\n",
        "In this section, the aim is to extend the classification efforts by incorporating additional available patient metadata (age, lesion location, and sex). The model performance evaluation will involve key metrics such as accuracy, specificity, sensitivity, precision, and F1-score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset and Dataloader\n",
        "\n",
        "The following dataset is compatible with this part of the project:\n",
        "\n",
        "- [ISIC2019](https://challenge.isic-archive.com/data/#2019):  Compiled dataset consisting of 25'331 dermatoscopic images in JPEG format with corresponding patient metadata entries (age, lesion location and sex) along with entries indicating gold standard malignant status (skin leasion classification in 9 different categories).\n",
        "\n",
        "Please note that for the purpose of this work, only binary classification was investigated (non-melanoma vs melanoma). Therefore, the analysis and training processes focused exclusively on the melanoma category of each dataset, considering the gold standard malignant status provided in the entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preparation and Formatting\n",
        "\n",
        "In this code excerpt, the script defines the path for the dataset (`DATASET_PATH`) and specifies the dataset to be used (`DATASET_USED`). If needed, the code includes an uncommented line to unzip the dataset using the `utils.unzip_data` function.\n",
        "\n",
        "Subsequently, the complete path to the dataset (`DATA_PATH`) is created, and another uncommented line is provided to format the dataset using the `utils.format_dataset_PART2` function. This step ensures that the dataset is prepared and formatted appropriately for subsequent machine learning tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the model module using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "# Define the path for the dataset and specify the dataset to be used\n",
        "DATASET_PATH = '/content/data' # Modify to match your folder structure. For Colab: '/content/data'.\n",
        "DATASET_USED = 'ISIC2019'\n",
        "SPLIT_RATIO = 0.2\n",
        "\n",
        "# Create the complete path to the dataset\n",
        "DATA_PATH = os.path.join(DATASET_PATH, DATASET_USED)\n",
        "\n",
        "# Delete data foders if they already exist\n",
        "utils.delete_folder(DATA_PATH)\n",
        "\n",
        "# Unzip the dataset\n",
        "utils.unzip_data(DATASET_PATH, DATASET_USED)\n",
        "\n",
        "# Split in train and test dataset according to the SPLIT_RATIO\n",
        "utils.split_folders(DATA_PATH, p_test=SPLIT_RATIO, type='classification', metadata=True, seed=11) \n",
        "\n",
        "# Format the dataset for processing\n",
        "utils.format_dataset(DATA_PATH, task='classification', test_folder=True, benchmark=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining the Segmentation Model\n",
        "\n",
        "This following code snippet, defines a pre-trained segmentation model, and initializes a trainer for that model. It allows you to choose between different segmentation models (simple_cnn, unet, or unet++) and set specific filter compositions. The code then instantiates the chosen model, and loads a pre-trained segmentation model for further use (prost-proceesing is automatically applied to segmentation predictions). Adjust the segmentation model choice and parameters as needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the model module using importlib\n",
        "import importlib\n",
        "importlib.reload(models)\n",
        "importlib.reload(training)\n",
        "\n",
        "# Define the chosen model and set the filter numbers\n",
        "SEG_MODEL = 'unet++' # Change according to the model you want to use as segmentor\n",
        "SEG_MODEL_NAME = f'{SEG_MODEL}.pt'\n",
        "filter_num = [8, 16, 32, 64, 128] # Set filter composition\n",
        "\n",
        "# Choose and initialize the specified model based on the selected architecture\n",
        "if SEG_MODEL == 'simple_cnn':\n",
        "    model_seg = models.SimpleCNN().to(device)\n",
        "elif SEG_MODEL == 'unet':\n",
        "    model_seg = models.UNet(filter_num).to(device)\n",
        "elif SEG_MODEL == 'unet++':\n",
        "    model_seg = models.UNetPlusPlus(filter_num).to(device)\n",
        "else:\n",
        "    raise ValueError('Please choose one of the following models: simple_cnn, unet, or unet++')\n",
        "\n",
        "# Initialize the segmentation trainner\n",
        "trainer_seg = training.Trainer(model_seg, device)\n",
        "\n",
        "# Load the pre-trained segmentation model\n",
        "trainer_seg.load_model(SEG_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Splitting\n",
        "\n",
        "In this code snippet, the batch size for data loading is set, and a `LesionDataset` is created for a classification test task. The dataset is then randomly split into training and testing sets with an given ratio. Finally, the sizes of the original, training, and test datasets are displayed for quick reference.\n",
        "\n",
        "`metadat=True`in the `dataset.LesionDataset` insure the metadata integration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the dataset module using importlib\n",
        "import importlib\n",
        "importlib.reload(dataset)\n",
        "\n",
        "# Create training and testing datasets using the LesionDataset class\n",
        "lesion_dataset_train = dataset.LesionDataset(DATA_PATH, TYPE='train', task='classification', trainer_seg=trainer_seg, metadata=True)\n",
        "lesion_dataset_test = dataset.LesionDataset(DATA_PATH, TYPE='test', task='classification', trainer_seg=trainer_seg, metadata=True)\n",
        "\n",
        "# Display the sizes of the original, training, and test datasets\n",
        "print(\"+\" + \"-\" * 30 + \"+\")\n",
        "print(f\"| Original dataset            | {len(lesion_dataset_train) + len(lesion_dataset_test)}{' ' * (20 - len(str(len(lesion_dataset_train) + len(lesion_dataset_test))))}\")\n",
        "print(f\"| Training dataset            | {len(lesion_dataset_train)}{' ' * (20 - len(str(len(lesion_dataset_train))))}\")\n",
        "print(f\"| Test dataset                | {len(lesion_dataset_test)}{' ' * (20 - len(str(len(lesion_dataset_test))))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Resampling Strategy and DataLoader Configuration\n",
        "\n",
        "This code segment defines and implements a resampling strategy for dataset balancing. Depending on the specified resampling strategy, the code creates DataLoaders either with or without the use of WeightedRandomSampler.\n",
        "\n",
        "- When `RESAMPLING` is set to `None`, DataLoaders are created without resampling, utilizing a regular shuffle strategy.\n",
        "- In the case of `RESAMPLING` being set to 'upsampling', a WeightedRandomSampler is employed to balance the dataset. The weights for each class are calculated, and a WeightedRandomSampler is created using these weights for the training set. A regular DataLoader is used for the test set.\n",
        "- `RESAMPLING_RATIO` can be used to manipulate the upsampling ratio (`RESAMPLING_RATIO = 1` corresponds to a 1:1 sampling).\n",
        "- The code concludes by displaying the label ratio in the training set and printing the label distribution in the first 5 batches of the training set.\n",
        "\n",
        "When selected, the upsampling strategy is exclusively applied to the training set to avoid any potential bias introduced during testing. The test set remains unchanged, ensuring the evaluation reflects the original distribution of classes. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the resampling strategy ('upsampling' or None) and the resampling ratio\n",
        "RESAMPLING = 'upsampling'\n",
        "RESAMPLING_RATIO = 1\n",
        "BATCH_SIZE = 16 # Set the batch size for data loading\n",
        "\n",
        "if RESAMPLING is None:\n",
        "    # No resampling - create DataLoader without WeightedRandomSampler\n",
        "    trainloader = torch.utils.data.DataLoader(lesion_dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(lesion_dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    \n",
        "elif RESAMPLING == 'upsampling':\n",
        "    # Upsampling strategy to balance the dataset using WeightedRandomSampler\n",
        "    # Get labels from the training dataset\n",
        "    labels = utils.get_labels(lesion_dataset_train)\n",
        "\n",
        "    # Count the number of samples in each class\n",
        "    class_sample_count = np.array([len(np.where(labels == t)[0]) for t in np.unique(labels)])\n",
        "\n",
        "    # Calculate weights for each class to balance the dataset\n",
        "    weight = 1. / class_sample_count\n",
        "    weight[-1] = weight[-1] * RESAMPLING_RATIO\n",
        "\n",
        "    # Compute sample weights for each instance in the training set\n",
        "    samples_weight = np.array([weight[t] for t in labels])\n",
        "    samples_weight = torch.from_numpy(samples_weight)\n",
        "\n",
        "    # Create a WeightedRandomSampler using the calculated weights\n",
        "    sampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n",
        "\n",
        "    # Create DataLoader with WeightedRandomSampler for training set and regular DataLoader for test set\n",
        "    trainloader = torch.utils.data.DataLoader(lesion_dataset_train, batch_size=BATCH_SIZE, sampler=sampler)\n",
        "    testloader = torch.utils.data.DataLoader(lesion_dataset_test, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "# Display the ratio of labels in the training set\n",
        "total_labels = len(labels)\n",
        "ratio_1 = len(np.where(labels == 1)[0])\n",
        "ratio_0 = len(np.where(labels == 0)[0])\n",
        "\n",
        "print(f\"Training label ratio '1'/'0': {ratio_1}/{ratio_0}\")\n",
        "\n",
        "# Print the label distribution in the first 5 batches of the training set\n",
        "print(\"Batch Index   | Ratio '1'   | Ratio '0'\")\n",
        "print(\"---------------------------------------\")\n",
        "for i, data in enumerate(itertools.islice(trainloader, 5)):\n",
        "    gt = data['label'].to(device).float()\n",
        "    ratio_1_batch = (gt == 1).sum().item()\n",
        "    ratio_0_batch = (gt == 0).sum().item()\n",
        "    print(f\"Batch {i+1:<2}        | {ratio_1_batch:<11} | {ratio_0_batch:<11}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Transformation\n",
        "\n",
        "The following transformations are applied to the training set:\n",
        "- `TF.hflip()`: Random horizontal flipping with a probability of 50%\n",
        "- `TF.vflip()`: Random vertical flipping with a probability of 50%\n",
        "- `T.RandomRotation()`: Random rotation up to 45 degrees with a probability of 50%\n",
        "- `T.RandomAffine()`: Random zoom with a maximum of 120% with a probability of 50%\n",
        "- `roi(seg_model)`: That extracts the region of interest from the original image using the segmentation model (or the groundtruth if `BENCHMARK_SEG == True`)\n",
        "- `add_hair()`: Add artificial hair with a probability of 50%\n",
        "- `transforms.ColorJitter()`: Adds controlled variability to the images by randomly adjusting their brightness, contrast, saturation, and hue with a probability of 50%\n",
        "- `T.RandomErasing()`: Applies random erasing to input images with a probability of 25%\n",
        "\n",
        "Note that proper resizing (224x224) is also performed to match the different model needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code snippet generates random indices for both the training and test datasets. Subsequently, it employs the `plot_data_PART2` function from the `plots` module to visualize data corresponding to the randomly selected indices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the dataset module using importlib\n",
        "import importlib\n",
        "importlib.reload(plots)\n",
        "\n",
        "index_train = random.randint(0, len(lesion_dataset_train) - 1)\n",
        "index_test = random.randint(0, len(lesion_dataset_test) - 1)\n",
        "\n",
        "# Plot the data using the plot_data function from the plot module\n",
        "plots.plot_data_PART2(lesion_dataset_train, lesion_dataset_test, index_train, index_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Classifier Model Configuration and Initialization\n",
        "\n",
        "In this section, the chosen model architecture with the option to customize the dropout rate.\n",
        "\n",
        "The script proceeds to initialize the specified model based on the chosen architecture (`MODEL`). Depending on the selected model, including options like 'random,' 'constant', or various ResNet configurations, the corresponding model instance is created and moved to the specified computing device.\n",
        "\n",
        "`metadata` is set to `True` for the integration of metadata in the models.\n",
        "\n",
        "It's crucial to note that users can seamlessly tailor the model selection to their specific requirements by adjusting the `MODEL` variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the model module using importlib\n",
        "import importlib\n",
        "importlib.reload(models)\n",
        "\n",
        "# Define the chosen model and set the filter numbers\n",
        "MODEL = 'resnet50_meta' # Change according to the model you want to train\n",
        "DROPOUT = 0.5\n",
        "\n",
        "# Choose and initialize the specified model based on the selected architecture\n",
        "if MODEL == 'random':\n",
        "    model = models.RandomGuessModel(task='classification').to(device)\n",
        "elif MODEL == 'constant':\n",
        "    model = models.ConstantModel(task='classification').to(device)\n",
        "elif 'resnet18_meta' in MODEL:\n",
        "    model = models.ResNet18(metadata=True, num_metadata_features=3, p_dropout=DROPOUT).to(device)\n",
        "elif 'resnet34_meta' in MODEL:\n",
        "    model = models.ResNet34(metadata=True, num_metadata_features=3, p_dropout=DROPOUT).to(device)\n",
        "elif 'resnet50_meta' in MODEL:\n",
        "    model = models.ResNet50(metadata=True, num_metadata_features=3, p_dropout=DROPOUT).to(device)\n",
        "elif 'resnet101_meta' in MODEL:\n",
        "    model = models.ResNet101(metadata=True, num_metadata_features=3, p_dropout=DROPOUT).to(device)\n",
        "else:\n",
        "    raise ValueError('Please choose one of the following models: random, constant, resnet18_meta, resnet34_meta, resnet50_meta or resnet101_meta.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training\n",
        "\n",
        "In this script users can specify whether to load a pre-trained model (`LOAD`) and set the model name accordingly.\n",
        "\n",
        "The Trainer is then initialized with the specified model and device, and based on the `LOAD` condition, either a new model is trained or a pre-trained model is loaded.\n",
        "\n",
        "- If `LOAD` is set to `False`, the script defines parameters for training, such as the number of epochs (`EPOCHS`), learning rate (`LEARNING_RATE`), weight decay (`WEIGHT_DECAY`), and gradient clipping (`CLIP_GRADIENT`). The model is then trained for the specified number of epochs unless it's the 'random' or 'constant' model. Training and testing losses are recorded.\n",
        "\n",
        "- If `LOAD` is set to `True`, a pre-trained model with the specified name (`MODEL_NAME`) is loaded using the `trainer.load_model` function.\n",
        "\n",
        "In either case, informative messages are printed, indicating the completion of the training process or the successful loading of a pre-trained model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the training module using importlib\n",
        "import importlib\n",
        "importlib.reload(training)\n",
        "\n",
        "# Specify whether or not to load the pre-trained model\n",
        "LOAD = False\n",
        "\n",
        "# Set the model name\n",
        "MODEL_NAME = f'{MODEL}.pt'\n",
        "\n",
        "# Initialize the Trainer with the specified model and device\n",
        "trainer = training.Trainer(model, device, task='classification')\n",
        "\n",
        "if LOAD == False:\n",
        "    # Set the number of training epochs\n",
        "    EPOCHS = 10\n",
        "    LEARNING_RATE = 5e-5\n",
        "    WEIGHT_DECAY = 0\n",
        "    CLIP_GRADIENT = False\n",
        "\n",
        "    # Train the model if it's not the random model\n",
        "    if MODEL != 'random' and MODEL != 'constant':\n",
        "      # Train the model for the specified number of epochs\n",
        "      losses_train, losses_test = trainer.train(EPOCHS, trainloader, testloader, metadata=True, model_name=MODEL_NAME, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY, clip_gradient=CLIP_GRADIENT)\n",
        "\n",
        "    # Display a message indicating the completion of the training process\n",
        "    print(f'Training finished!')\n",
        "\n",
        "else:\n",
        "    # Load a pre-trained model\n",
        "    trainer.load_model(MODEL_NAME)\n",
        "    print(f'Pre-trained model {MODEL_NAME} loaded successfuly.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loss Visualization\n",
        "\n",
        "This section checks if the selected model is not a benchmark model (neither 'random' nor 'constant'). \n",
        "\n",
        "- If the model is not a benchmark and training is not based on a pre-trained model (`LOAD == False`), the script plots the training and testing loss curves using the `plot_loss` function from the `plots` module.\n",
        "\n",
        "- For benchmark models ('random' or 'constant') or pre-trained models, a message is printed indicating that loss plots are not available.\n",
        "\n",
        "This visualization aids in assessing the training progress and performance of the model over epochs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the plot module using importlib\n",
        "import importlib\n",
        "importlib.reload(plots)\n",
        "\n",
        "# Check if the model is not a benchmark model (neither random nor constant)\n",
        "if MODEL != 'random' and MODEL != 'constant':\n",
        "  if LOAD == False:\n",
        "    # Plot the training and testing loss curves using the plot_loss_unet function\n",
        "    plots.plot_loss(losses_train, losses_test, save_path=None)\n",
        "  else:\n",
        "    # If a pre-trained model is loaded, recover train and test epoch losses\n",
        "    losses_train = []\n",
        "    losses_test  = []\n",
        "\n",
        "    # Plot the training and testing loss curves using the plot_loss_unet function\n",
        "    plots.plot_loss(losses_train, losses_test, save_path=None)\n",
        "else:\n",
        "  # Print a message indicating that loss plots are not available for benchmark models\n",
        "  print('Loss plot not available for benchmark models.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Model Evaluation and Performance Scores\n",
        "\n",
        "This script checks whether the selected model is not a benchmark model (neither 'random' nor 'constant').\n",
        "\n",
        "- If the model is not a benchmark, the script proceeds to load the state dictionary from the saved model. Depending on the availability of a GPU, the model is loaded directly if a GPU is present; otherwise, it is loaded onto the CPU using `map_location`.\n",
        "\n",
        "- Subsequently, the model's performance is evaluated, and scores are obtained using the `get_scores_CLASS` function from the `utils` module. The evaluation involves comparing model predictions on the test dataset (`lesion_dataset_test`) against the ground truth labels. The specified threshold (`THRESHOLD`) is used to classify predictions.\n",
        "\n",
        "This process provides valuable insights into the model's classification performance and aids in assessing its effectiveness on unseen data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reload the utils module using importlib\n",
        "import importlib\n",
        "importlib.reload(utils)\n",
        "\n",
        "THRESHOLD = 0.5\n",
        "\n",
        "# Check if the model is not a benchmark model (neither random nor constant)\n",
        "if MODEL != 'random' and MODEL != 'constant':\n",
        "  # Load the state dictionary from the saved model, use map_location to load on the CPU\n",
        "  model = torch.load(MODEL_NAME, map_location=torch.device('cpu'))\n",
        "\n",
        "# Evaluate the model's performance and obtain scores using the get_scores function\n",
        "utils.get_scores_CLASS(trainer, lesion_dataset_test, dataset_train=None, threshold=THRESHOLD) # Set 'dataset_train = lesion_dataset_train' to get the training metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset Citation\n",
        "\n",
        "(comply with the attribution requirements of the CC-BY-NC license)\n",
        "\n",
        "[1] Gutman, David; Codella, Noel C. F.; Celebi, Emre; Helba, Brian; Marchetti, Michael; Mishra, Nabin; Halpern, Allan. \"Skin Lesion Analysis toward Melanoma Detection: A Challenge at the International Symposium on Biomedical Imaging (ISBI) 2016, hosted by the International Skin Imaging Collaboration (ISIC)\". eprint arXiv:1605.01397. 2016.\n",
        "\n",
        "[2] Codella N, Gutman D, Celebi ME, Helba B, Marchetti MA, Dusza S, Kalloo A, Liopyris K, Mishra N, Kittler H, Halpern A. \"Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC)\". arXiv: 1710.05006 [cs.CV]\n",
        "\n",
        "[3] Tschandl, P., Rosendahl, C. & Kittler, H. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. Sci. Data 5, 180161 doi:10.1038/sdata.2018.161 (2018).\n",
        "\n",
        "[2] Noel C. F. Codella, David Gutman, M. Emre Celebi, Brian Helba, Michael A. Marchetti, Stephen W. Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, Allan Halpern: \"Skin Lesion Analysis Toward Melanoma Detection: A Challenge at the 2017 International Symposium on Biomedical Imaging (ISBI), Hosted by the International Skin Imaging Collaboration (ISIC)\", 2017; arXiv:1710.05006.\n",
        "\n",
        "[3] Marc Combalia, Noel C. F. Codella, Veronica Rotemberg, Brian Helba, Veronica Vilaplana, Ofer Reiter, Allan C. Halpern, Susana Puig, Josep Malvehy: \"BCN20000: Dermoscopic Lesions in the Wild\", 2019; arXiv:1908.02288."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a34d05b3-efab-43e7-8695-192c3c044280",
        "XJWiMrjqwnZo",
        "sLF3UV1LkBEM",
        "el-p5itWwZCN"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
